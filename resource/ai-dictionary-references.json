[
  {
    "url": "https://arxiv.org/abs/2005.11401",
    "desc": "Abstract page for arXiv paper 2005.11401: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "title": "[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://research.ibm.com/blog/retrieval-augmented-generation-RAG",
    "desc": "RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI&rsquo;s decision making process.",
    "title": "What is retrieval-augmented generation (RAG)? - IBM Research",
    "auth": "@IBMResearch",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2212.05773",
    "desc": "Abstract page for arXiv paper 2212.05773: A Survey on Natural Language Processing for Programming",
    "title": "[2212.05773] A Survey on Natural Language Processing for Programming",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2307.02503",
    "desc": "Abstract page for arXiv paper 2307.02503: Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "title": "[2307.02503] Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://galileo.ai/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Explore ThoT, CoN, CoVe, emotional prompts, and ExpertPrompting for precise language models in RAG applications. Stay ahead in the dynamic RAG landscape with reliable insights. Dive in now!",
    "title": "RAG LLM Prompting Techniques to Reduce Hallucinations",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2311.09210",
    "desc": "Abstract page for arXiv paper 2311.09210: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "title": "[2311.09210] Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://en.wikipedia.org/wiki/Backpropagation",
    "desc": "Backpropagation - Wikipedia",
    "title": "Backpropagation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "When selecting the best large language models for your organisation&rsquo;s needs, there are many factors to consider. Undoubtedly, with there being a strong correlation between a model&rsquo;s parameter count, looking at the size of an LLM is a wise strategy. Similarly, you might look at its performance at common benchmark or inference performance tests &ndash; [&hellip;]",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/1706.03762",
    "desc": "Abstract page for arXiv paper 1706.03762: Attention Is All You Need",
    "title": "[1706.03762] Attention Is All You Need",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6",
    "desc": "Using LLM&rsquo;s for Retrieval and Reranking",
    "title": "Using LLM&rsquo;s for Retrieval and Reranking",
    "auth": "Jerry Liu",
    "date": null
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models",
    "desc": "machine learning - What is the temperature in the GPT models? - Artificial Intelligence Stack Exchange",
    "title": "machine learning - What is the temperature\" in the GPT models? - Artificial Intelligence Stack Exchange",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "When selecting the best large language models for your organisation&rsquo;s needs, there are many factors to consider. Undoubtedly, with there being a strong correlation between a model&rsquo;s parameter count, looking at the size of an LLM is a wise strategy. Similarly, you might look at its performance at common benchmark or inference performance tests &ndash; [&hellip;]",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://scribe.rip/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275",
    "desc": "Simplified Math behind Dropout in Deep Learning",
    "title": "Simplified Math behind Dropout in Deep Learning",
    "auth": "Chitta Ranjan",
    "date": 980985600
  },
  {
    "url": "https://scribe.rip/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d/",
    "desc": "8 Simple Techniques to Prevent Overfitting",
    "title": "8 Simple Techniques to Prevent Overfitting",
    "auth": "cant extract from medium",
    "date": 993942000
  },
  {
    "url": "https://ar5iv.labs.arxiv.org/html/2310.04415",
    "desc": "Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that t&hellip;",
    "title": "[2310.04415] Why Do We Need Weight Decay in Modern Deep Learning?",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "When selecting the best large language models for your organisation&rsquo;s needs, there are many factors to consider. Undoubtedly, with there being a strong correlation between a model&rsquo;s parameter count, looking at the size of an LLM is a wise strategy. Similarly, you might look at its performance at common benchmark or inference performance tests &ndash; [&hellip;]",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@mccartni/implications-of-batch-size-on-llm-training-and-inference-3320cb48d610",
    "desc": "Implications of Batch Size on LLM training and inference",
    "title": "Implications of Batch Size on LLM training and inference",
    "auth": "Nick McCarthy",
    "date": null
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Explore ThoT, CoN, CoVe, emotional prompts, and ExpertPrompting for precise language models in RAG applications. Stay ahead in the dynamic RAG landscape with reliable insights. Dive in now!",
    "title": "RAG LLM Prompting Techniques to Reduce Hallucinations",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/think/topics/ai-model",
    "desc": "An AI model is a program that applies one or more algorithms to data to recognize patterns, make predictions or make decisions without human intervention.",
    "title": "What Is an AI Model? | IBM",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gradient",
    "desc": "Gradient - Wikipedia",
    "title": "Gradient - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "desc": "Machine learning - Wikipedia",
    "title": "Machine learning - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@amanzanocontreras/machine-learning-and-deep-learning-in-short-multilayer-perceptron-9bb3f061dff5",
    "desc": "Machine Learning and Deep Learning in short: Multilayer Perceptron",
    "title": "Machine Learning and Deep Learning in short: Multilayer Perceptron",
    "auth": "Angel Gabriel Manzano Contreras",
    "date": 980985600
  },
  {
    "url": "https://en.wikipedia.org/wiki/Logistic_regression",
    "desc": "Logistic regression - Wikipedia",
    "title": "Logistic regression - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier",
    "desc": "Naive Bayes classifier - Wikipedia",
    "title": "Naive Bayes classifier - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Neural_network_%28machine_learning%29",
    "desc": "Neural network (machine learning) - Wikipedia",
    "title": "Neural network (machine learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Neural_scaling_law",
    "desc": "Neural scaling law - Wikipedia",
    "title": "Neural scaling law - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Byte-pair_encoding",
    "desc": "Byte-pair encoding - Wikipedia",
    "title": "Byte-pair encoding - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2302.01560v2",
    "desc": "2302.01560v2",
    "title": "2302.01560v2",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2303.11366",
    "desc": "Abstract page for arXiv paper 2303.11366: Reflexion: Language Agents with Verbal Reinforcement Learning",
    "title": "[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Rectified_flow",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Score_matching",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search",
    "desc": "Monte Carlo tree search - Wikipedia",
    "title": "Monte Carlo tree search - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Thermodynamic_beta",
    "desc": "Thermodynamic beta - Wikipedia",
    "title": "Thermodynamic beta - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Convolution",
    "desc": "Convolution - Wikipedia",
    "title": "Convolution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Stochastic",
    "desc": "Stochastic - Wikipedia",
    "title": "Stochastic - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mathematical_induction",
    "desc": "Mathematical induction - Wikipedia",
    "title": "Mathematical induction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Deterministic_algorithm",
    "desc": "Deterministic algorithm - Wikipedia",
    "title": "Deterministic algorithm - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",
    "desc": "Kullback&ndash;Leibler divergence - Wikipedia",
    "title": "Kullback&ndash;Leibler divergence - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Normal_distribution",
    "desc": "Normal distribution - Wikipedia",
    "title": "Normal distribution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Cosine_similarity",
    "desc": "Cosine similarity - Wikipedia",
    "title": "Cosine similarity - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/building-the-open-data-stack/a-vector-primer-understanding-the-basics-of-generative-ai-28ff4c1934a7",
    "desc": "A Vector Primer: Understanding the Basics of Generative AI",
    "title": "A Vector Primer: Understanding the Basics of Generative AI",
    "auth": "DataStax",
    "date": null
  },
  {
    "url": "https://en.wikipedia.org/wiki/Vector_notation",
    "desc": "Vector notation - Wikipedia",
    "title": "Vector notation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gaussian_splatting",
    "desc": "Gaussian splatting - Wikipedia",
    "title": "Gaussian splatting - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inverted_index",
    "desc": "Inverted index - Wikipedia",
    "title": "Inverted index - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/similarity-search-with-ivfpq-9c6348fd4db3",
    "desc": "Similarity Search with IVFPQ",
    "title": "Similarity Search with IVFPQ",
    "auth": "Peggy Chang",
    "date": null
  },
  {
    "url": "https://en.wikipedia.org/wiki/Cell-probe_model#Approximate_Nearest_Neighbor_Searching",
    "desc": "Cell-probe model - Wikipedia",
    "title": "Cell-probe model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/html/2404.19284v2",
    "desc": "Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation",
    "title": "Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.elastic.co/blog/understanding-ann",
    "desc": "Discover the power of approximate nearest neighbor (ANN) algorithms. ANN search is the key to lightning-fast searches and valuable insights in vast data landscapes....",
    "title": "Understanding the approximate nearest neighbor (ANN) algorithm | Elastic Blog",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://ieeexplore.ieee.org/document/9942356/metrics#metrics",
    "desc": "Approximate nearest neighbour search (ANNS) in high-dimensional space is an essential and fundamental operation in many applications from many domains such as multimedia database, information retrieval and computer vision. With the rapidly growing volume of data and the dramatically increasing demands of users, traditional heuristic-based ANNS solutions have been facing great challenges in terms of both efficiency and accuracy. Inspired by the recent successes of deep learning in many fields, su",
    "title": "Deep Learning for Approximate Nearest Neighbour Search: A Survey and Future Directions | IEEE Journals &amp; Magazine | IEEE Xplore",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world",
    "desc": "Hierarchical navigable small world - Wikipedia",
    "title": "Hierarchical navigable small world - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://sds-aau.github.io/M3Port19/portfolio/ann/",
    "desc": "Approximate Nearest Neighbors Oh Yeah (ANNOY) - AAU Social Data Science Deep Learning - 2019 Portfolio",
    "title": "Approximate Nearest Neighbors Oh Yeah (ANNOY) - AAU Social Data Science Deep Learning - 2019 Portfolio",
    "auth": "M3Port19",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@abdullahsamilguser/approximate-nearest-neighbor-methods-713dcfa8518f",
    "desc": "Approximate Nearest Neighbor Methods",
    "title": "Approximate Nearest Neighbor Methods",
    "auth": "cant extract from medium",
    "date": null
  },
  {
    "url": "https://www.algolia.com/blog/ai/what-is-vector-search",
    "desc": "This blog offers an introduction to vector search and some of the technology behind it such as vector embeddings and neural networks.",
    "title": "What is vector search? - Algolia Blog | Algolia",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/gsi-technology/ann-benchmarks-a-data-scientists-journey-to-billion-scale-performance-db191f043a27",
    "desc": "ANN Benchmarks: A Data Scientist&rsquo;s Journey to Billion Scale Performance",
    "title": "ANN Benchmarks: A Data Scientist&rsquo;s Journey to Billion Scale Performance",
    "auth": "Braden Riggs",
    "date": 978307200
  },
  {
    "url": "https://research.google/blog/announcing-scann-efficient-vector-similarity-search/",
    "desc": "Posted by Philip Sun, Software Engineer, Google Research Suppose one wants to search through a large dataset of literary works using queries that r...",
    "title": "Announcing ScaNN: Efficient Vector Similarity Search",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://research.google/blog/soar-new-algorithms-for-even-faster-vector-search-with-scann/",
    "desc": "SOAR: New algorithms for even faster vector search with ScaNN",
    "title": "SOAR: New algorithms for even faster vector search with ScaNN",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb",
    "desc": "Based on established Google technology, the new ScaNN index in AlloyDB delivers higher performance than the HNSW index in standard PostgreSQL.",
    "title": "Understanding the SCaNN index in AlloyDB | Google Cloud Blog",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@DataPlayer/scalable-approximate-nearest-neighbour-search-using-googles-scann-and-facebook-s-faiss-3e84df25ba",
    "desc": "Scalable Approximate Nearest Neighbour Search using Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS.",
    "title": "Scalable Approximate Nearest Neighbour Search using Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS.",
    "auth": "ANURAG DIXIT",
    "date": null
  },
  {
    "url": "https://arxiv.org/pdf/2401.08281",
    "desc": "2401.08281",
    "title": "2401.08281",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/understanding-faiss-619bb6db2d1a",
    "desc": "Understanding FAISS",
    "title": "Understanding FAISS",
    "auth": "Vedashree Patil",
    "date": 996620400
  },
  {
    "url": "https://scribe.rip/@DataPlayer/scalable-approximate-nearest-neighbour-search-using-googles-scann-and-facebook-s-faiss-3e84df25ba",
    "desc": "Scalable Approximate Nearest Neighbour Search using Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS.",
    "title": "Scalable Approximate Nearest Neighbour Search using Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS.",
    "auth": "ANURAG DIXIT",
    "date": null
  },
  {
    "url": "https://github.com/yahoojapan/NGT/wiki/Command-Quick-Referenc",
    "desc": "Nearest Neighbor Search with Neighborhood Graph and Tree for High-dimensional Data - Home &middot; yahoojapan/NGT Wiki",
    "title": "Home &middot; yahoojapan/NGT Wiki &middot; GitHub",
    "auth": "yahoojapan",
    "date": 0
  },
  {
    "url": "https://opensource.com/article/19/10/ngt-open-source-library",
    "desc": "Approximate nearest neighbor (ANN) search is used in deep l",
    "title": "NGT: A library for high-speed approximate nearest neighbor search | Opensource.com",
    "auth": "Masajiro Iwasaki",
    "date": 0
  },
  {
    "url": "https://dl.acm.org/doi/abs/10.5555/3504035.3505114",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://spcl.inf.ethz.ch/Publications/.pdf/wenqi_fanns_slides.pdf",
    "desc": "Cannot extract meta-data from PDF at present",
    "title": "wenqi_fanns_slides.pdf",
    "auth": "Unknown",
    "date": 0
  },
  {
    "url": "https://dl.acm.org/doi/10.1145/3581784.3607045",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Hamming_distance",
    "desc": "Hamming distance - Wikipedia",
    "title": "Hamming distance - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Frobenius_inner_product",
    "desc": "Frobenius inner product - Wikipedia",
    "title": "Frobenius inner product - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Dot_product",
    "desc": "Dot product - Wikipedia",
    "title": "Dot product - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inner_product_space",
    "desc": "Inner product space - Wikipedia",
    "title": "Inner product space - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mahalanobis_distance",
    "desc": "Mahalanobis distance - Wikipedia",
    "title": "Mahalanobis distance - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://lantern.dev/blog/pq",
    "desc": "We implemented product quantization in Lantern and benchmarked it using the LAION 100M 768-dimensional vector dataset.",
    "title": "Product Quantization in Postgres | Lantern Blog",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.pinecone.io/learn/series/faiss/product-quantization/",
    "desc": "Product quantization (PQ) is a popular method for dramatically compressing high-dimensional vectors to use 97% less memory, and for making nearest-neighbor search speeds 5.5x faster in our tests.",
    "title": "Product Quantization: Compressing high-dimensional vectors by 97% | Pinecone",
    "auth": "@pinecone",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing",
    "desc": "Locality-sensitive hashing - Wikipedia",
    "title": "Locality-sensitive hashing - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/K-d_tree",
    "desc": "k-d tree - Wikipedia",
    "title": "k-d tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://ieeexplore.ieee.org/document/8807277",
    "desc": "One of the best-known and most effective methods in supervised classification is the k-nearest neighbors algorithm (kNN). Several approaches have been proposed to improve its accuracy, where fuzzy approaches prove to be among the most successful, highlighting the classical fuzzy k-nearest neighbors (FkNN). However, these traditional algorithms fail to tackle the large amounts of data that are available today. There are multiple alternatives to enable kNN classification in big datasets, spotlight",
    "title": "Fast and Scalable Approaches to Accelerate the Fuzzy k-Nearest Neighbors Classifier for Big Data | IEEE Journals &amp; Magazine | IEEE Xplore",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Ball_tree",
    "desc": "Ball tree - Wikipedia",
    "title": "Ball tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@ravuri.saitarun/kd-tree-ball-tree-algorithms-83271c4d1cd0",
    "desc": "KD-Tree &amp; Ball Tree Algorithms",
    "title": "KD-Tree &amp; Ball Tree Algorithms",
    "auth": "R K R Saitarun",
    "date": null
  },
  {
    "url": "https://scribe.rip/@polkamnithyasri/kd-tree-and-ball-tree-algorithms-aa99dfad4ceb",
    "desc": "Kd -Tree and Ball Tree Algorithms",
    "title": "Kd -Tree and Ball Tree Algorithms",
    "auth": "NithyaSri",
    "date": null
  },
  {
    "url": "https://proceedings.neurips.cc/paper_files/paper/2004/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "desc": "Cannot extract meta-data from PDF at present",
    "title": "1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "auth": "Unknown",
    "date": 0
  },
  {
    "url": "https://project.inria.fr/gudhi/files/2017/04/clement-jamin.pdf",
    "desc": "Cannot extract meta-data from PDF at present",
    "title": "clement-jamin.pdf",
    "auth": "Unknown",
    "date": 0
  },
  {
    "url": "https://patents.google.com/patent/US7539657B1/en",
    "desc": "One embodiment of the present invention provides a method and a system for building a parallel hybrid spill tree to facilitate parallel nearest-neighbor matching operations. During operation, the system receives a set of objects to be stored in the parallel hybrid spill tree. The system selects a subset of objects from the set of objects, and then uses this subset to create a &ldquo;top tree.&rdquo; Each node in the top tree defines an associated partition for the parallel hybrid, spill tree. The system use",
    "title": "US7539657B1 - Building parallel hybrid spill trees to facilitate parallel nearest-neighbor matching operations \n        - Google Patents",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/B%2B_tree",
    "desc": "B+ tree - Wikipedia",
    "title": "B+ tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/B-tree",
    "desc": "B-tree - Wikipedia",
    "title": "B-tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Robust_optimization",
    "desc": "Robust optimization - Wikipedia",
    "title": "Robust optimization - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2410.00690",
    "desc": "Abstract page for arXiv paper 2410.00690: Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "title": "[2410.00690] Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2406.16571",
    "desc": "Abstract page for arXiv paper 2406.16571: Differentiable Distributionally Robust Optimization Layers",
    "title": "[2406.16571] Differentiable Distributionally Robust Optimization Layers",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2401.14655",
    "desc": "Abstract page for arXiv paper 2401.14655: Distributionally Robust Optimization and Robust Statistics",
    "title": "[2401.14655] Distributionally Robust Optimization and Robust Statistics",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Continuous_function",
    "desc": "Continuous function - Wikipedia",
    "title": "Continuous function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time",
    "desc": "Discrete time and continuous time - Wikipedia",
    "title": "Discrete time and continuous time - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Markov_chain",
    "desc": "Markov chain - Wikipedia",
    "title": "Markov chain - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wasserstein_metric",
    "desc": "Wasserstein metric - Wikipedia",
    "title": "Wasserstein metric - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Uniform_convergence_in_probability",
    "desc": "Uniform convergence in probability - Wikipedia",
    "title": "Uniform convergence in probability - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Sigmoid_function",
    "desc": "Sigmoid function - Wikipedia",
    "title": "Sigmoid function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Logistic_function",
    "desc": "Logistic function - Wikipedia",
    "title": "Logistic function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods",
    "desc": "Variational Bayesian methods - Wikipedia",
    "title": "Variational Bayesian methods - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Reparameterization_trick#Variational_inference",
    "desc": "Reparameterization trick - Wikipedia",
    "title": "Reparameterization trick - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://stackoverflow.com/questions/59868132/what-does-backbone-mean-in-a-neural-network#",
    "desc": "deep learning - What does backbone mean in a neural network? - Stack Overflow",
    "title": "deep learning - What does backbone mean in a neural network? - Stack Overflow",
    "auth": "No author for Q&amp;A sites",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Forward_diffusion_process",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Backward_diffusion_process_2",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation",
    "desc": "Fokker&ndash;Planck equation - Wikipedia",
    "title": "Fokker&ndash;Planck equation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wiener_process",
    "desc": "Wiener process - Wikipedia",
    "title": "Wiener process - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Brownian_motion",
    "desc": "Brownian motion - Wikipedia",
    "title": "Brownian motion - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Langevin_equation",
    "desc": "Langevin equation - Wikipedia",
    "title": "Langevin equation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Laplace_operator",
    "desc": "Laplace operator - Wikipedia",
    "title": "Laplace operator - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Discrete_mathematics",
    "desc": "Discrete mathematics - Wikipedia",
    "title": "Discrete mathematics - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Linearity",
    "desc": "Linearity - Wikipedia",
    "title": "Linearity - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.blopig.com/blog/2023/10/understanding-positional-encoding-in-transformers/",
    "desc": "Understanding positional encoding in Transformers | Oxford Protein Informatics Group",
    "title": "Understanding positional encoding in Transformers | Oxford Protein Informatics Group",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://ahmad-mustapha.scribe.rip/transformers-well-explained-positional-encoding-c8350904444f",
    "desc": "Transformers Well Explained: Positional Encoding",
    "title": "Transformers Well Explained: Positional Encoding",
    "auth": "Ahmad Mustapha",
    "date": 946684800
  },
  {
    "url": "https://scribe.rip/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b",
    "desc": "Positional Encoding Explained: A Deep Dive into Transformer PE",
    "title": "Positional Encoding Explained: A Deep Dive into Transformer PE",
    "auth": "Nikhil Chowdary Paleti",
    "date": 983404800
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
    "desc": "Gated recurrent unit - Wikipedia",
    "title": "Gated recurrent unit - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Maurice_Tweedie",
    "desc": "Maurice Tweedie - Wikipedia",
    "title": "Maurice Tweedie - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution",
    "desc": "Inverse Gaussian distribution - Wikipedia",
    "title": "Inverse Gaussian distribution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_statistics",
    "desc": "Maxwell&ndash;Boltzmann statistics - Wikipedia",
    "title": "Maxwell&ndash;Boltzmann statistics - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Brownian_motion",
    "desc": "Brownian motion - Wikipedia",
    "title": "Brownian motion - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods",
    "desc": "Variational Bayesian methods - Wikipedia",
    "title": "Variational Bayesian methods - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2401.12418",
    "desc": "Abstract page for arXiv paper 2401.12418: Towards Improved Variational Inference for Deep Bayesian Models",
    "title": "[2401.12418] Towards Improved Variational Inference for Deep Bayesian Models",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Scoring_rule#Hyv%C3%A4rinen_scoring_rule",
    "desc": "Scoring rule - Wikipedia",
    "title": "Scoring rule - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Simulated_annealing",
    "desc": "Simulated annealing - Wikipedia",
    "title": "Simulated annealing - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Annealing_%28materials_science%29",
    "desc": "Annealing (materials science) - Wikipedia",
    "title": "Annealing (materials science) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method",
    "desc": "Euler&ndash;Maruyama method - Wikipedia",
    "title": "Euler&ndash;Maruyama method - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Noisy_channel_model",
    "desc": "Noisy channel model - Wikipedia",
    "title": "Noisy channel model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@cloudswarup/the-building-blocks-of-llms-vectors-tokens-and-embeddings-1cd61cd20e35",
    "desc": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "title": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "auth": "Swarup Das",
    "date": null
  },
  {
    "url": "https://cohere.com/blog/llm-parameters-best-outputs-language-ai",
    "desc": "When using Language AI to generate content, there are many options to control the outputs. Lets take a look at them in this post.",
    "title": "LLM Parameters Demystified: Getting The Best Outputs from Language AI",
    "auth": "@cohere",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://byronsalty.scribe.rip/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "My (Growing) List of AI and LLM terminology",
    "title": "My (Growing) List of AI and LLM terminology",
    "auth": "Byron Salty",
    "date": 993942000
  },
  {
    "url": "https://en.wikipedia.org/wiki/FastText",
    "desc": "fastText - Wikipedia",
    "title": "fastText - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://blog.reachsumit.com/posts/2020/07/spell-checker-fasttext/",
    "desc": "Word vector representations with subword information are great for NLP modeling. But can we make lexical corrections using a trained embeddings space? Can its accuracy be high enough to beat Peter Norvigs spell-corrector? Let's find out!",
    "title": "Building a spell-checker with FastText word embeddings - Sumits Diary",
    "auth": "@_reachsumit",
    "date": 0
  },
  {
    "url": "https://norvig.com/spell-correct.html",
    "desc": "How to Write a Spelling Corrector",
    "title": "How to Write a Spelling Corrector",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/1607.04606",
    "desc": "Abstract page for arXiv paper 1607.04606: Enriching Word Vectors with Subword Information",
    "title": "[1607.04606] Enriching Word Vectors with Subword Information",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Parametric_model",
    "desc": "Parametric model - Wikipedia",
    "title": "Parametric model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/#2-2-parametric-vs-non-parametric-memory",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://lagstill.scribe.rip/rag-analytics-explored-8d389978880f",
    "desc": "RAG Analytics Explored",
    "title": "RAG Analytics Explored",
    "auth": "Alagu Prakalya P",
    "date": 978307200
  },
  {
    "url": "https://scribe.rip/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5",
    "desc": "Context Windows: The Short-term Memory of Large Language Models",
    "title": "Context Windows: The Short-term Memory of Large Language Models",
    "auth": "Chandler K",
    "date": 946684800
  },
  {
    "url": "https://swimm.io/learn/large-language-models/llm-context-windows-basics-examples-and-prompting-best-practices",
    "desc": "A context window refers to the amount of text data a language model can consider at one time when generating responses.",
    "title": "LLM Context Windows: Basics, Examples &amp; Prompting Best Practices",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://towardsai.net/p/data-science/pause-for-performance-the-guide-to-using-early-stopping-in-ml-and-dl-model-training",
    "desc": "Author(s): Shivamshinde Originally published on Towards AI. Photo by Aleksandr Kadykov on UnsplashTable of ContentIntroduction- What are Bias and Variance?- ...",
    "title": "Pause for Performance: The Guide to Using Early Stopping in ML and DL Model Training | Towards AI",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://news.ycombinator.com/item?id=35666201",
    "desc": "HTTP_ERROR, Unspecified failure https://news.ycombinator.com/item?id=35666201",
    "title": "HTTP_ERROR, Unspecified failure https://news.ycombinator.com/item?id=35666201",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.whytryai.com/p/zero-shot-one-shot-few-shot-prompting",
    "desc": "Zero-Shot, One-Shot, Few-Shot, More?",
    "title": "Zero-Shot, One-Shot, Few-Shot, More?",
    "auth": "Daniel Nest",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Confusion_matrix",
    "desc": "Confusion matrix - Wikipedia",
    "title": "Confusion matrix - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://davpinto.github.io/fastknn/articles/knn-extraction.html",
    "desc": "Feature Extraction with KNN &bull; fastknn",
    "title": "Feature Extraction with KNN &bull; fastknn",
    "auth": "fastknn",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
    "desc": "Dimensionality reduction - Wikipedia",
    "title": "Dimensionality reduction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Data_reduction",
    "desc": "Data reduction - Wikipedia",
    "title": "Data reduction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Decision_boundary",
    "desc": "Decision boundary - Wikipedia",
    "title": "Decision boundary - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Likelihood-ratio_test",
    "desc": "Likelihood-ratio test - Wikipedia",
    "title": "Likelihood-ratio test - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://datascience.fm/understanding-byte-pair-encoding-bpe-in-large-language-models-llms/",
    "desc": "This fixed vocabulary constraint becomes particularly problematic for languages with complex word formation processes like agglutination and compounding, which can create a vast number of word variations that a fixed-size vocabulary cant cover​​.",
    "title": "Understanding Byte Pair Encoding (BPE) in Large Language Models (LLMs)",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2306.08543v1",
    "desc": "2306.08543v1",
    "title": "2306.08543v1",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://nlp.stanford.edu/projects/glove/",
    "desc": "GloVe: Global Vectors for Word Representation",
    "title": "GloVe: Global Vectors for Word Representation",
    "auth": "Jeffrey Pennington",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "desc": "Recurrent neural network - Wikipedia",
    "title": "Recurrent neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
    "desc": "Convolutional neural network - Wikipedia",
    "title": "Convolutional neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
    "desc": "Long short-term memory - Wikipedia",
    "title": "Long short-term memory - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/think/topics/parameter-efficient-fine-tuning",
    "desc": "Parameter-efficient fine-tuning (PEFT) is a method of improving the performance of pretrained large language models (LLMs) and neural networks for specific tasks or data sets.",
    "title": "What is parameter-efficient fine-tuning (PEFT)? | IBM",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2106.09685",
    "desc": "Abstract page for arXiv paper 2106.09685: LoRA: Low-Rank Adaptation of Large Language Models",
    "title": "[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29#Low-rank_adaptation",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/BERT_%28language_model%29",
    "desc": "BERT (language model) - Wikipedia",
    "title": "BERT (language model) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
    "desc": "Mixture of experts - Wikipedia",
    "title": "Mixture of experts - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2310.06117",
    "desc": "Abstract page for arXiv paper 2310.06117: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
    "title": "[2310.06117] Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@akriti.upadhyay/enhancing-llms-reasoning-with-step-back-prompting-47fad1cf5968",
    "desc": "Enhancing LLM&rsquo;s Reasoning with Step Back Prompting",
    "title": "Enhancing LLM&rsquo;s Reasoning with Step Back Prompting",
    "auth": "Akriti Upadhyay",
    "date": 946684800
  },
  {
    "url": "https://scribe.rip/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83",
    "desc": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "title": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "auth": "azhar",
    "date": null
  },
  {
    "url": "https://github.com/paperswithcode",
    "desc": "Papers with code has 13 repositories available. Follow their code on GitHub.",
    "title": "Papers with code &middot; GitHub",
    "auth": "paperswithcode",
    "date": 0
  },
  {
    "url": "https://github.com/paperswithcode",
    "desc": "Papers with code has 13 repositories available. Follow their code on GitHub.",
    "title": "Papers with code &middot; GitHub",
    "auth": "paperswithcode",
    "date": 0
  },
  {
    "url": "https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data",
    "desc": "If trends continue, language models will fully utilize the stock of human-generated public text between 2026 and 2032.",
    "title": "Will We Run Out of Data to Train Large Language Models? | Epoch AI",
    "auth": "Pablo Villalobos",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2211.04325",
    "desc": "2211.04325",
    "title": "2211.04325",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.fast.ai/posts/2023-09-04-learning-jumps/",
    "desc": "We&rsquo;ve noticed an unusual training pattern in fine-tuning LLMs. At first we thought it&rsquo;s a bug, but now we think it shows LLMs can learn effectively from a single example.",
    "title": "Can LLMs learn from a single example? &ndash; fast.ai",
    "auth": "Jeremy Howard and Jonathan Whitaker",
    "date": 0
  },
  {
    "url": "https://www.coursera.org/collections/llms-terms",
    "desc": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "title": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://byronsalty.scribe.rip/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "My (Growing) List of AI and LLM terminology",
    "title": "My (Growing) List of AI and LLM terminology",
    "auth": "Byron Salty",
    "date": 993942000
  },
  {
    "url": "https://toloka.ai/blog/history-of-llms/",
    "desc": "The impressive speed at which AI has evolved has never been more apparent than it is now, with ChatGPT making headlines and the dramatic evolution of Large Language Models (LLMs) ever present in the media cycle. Millions of people worldwide have wasted no time adopting conversational AI tools in their day-to-day existence. These tools have not only enamored but also terrified audiences with their striking capabilities and efficiency and their potentially dangerous implications if not regulated w",
    "title": "The history, timeline, and future of LLMs",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.scribd.com/document/663506964/compact-guide-to-large-language-models",
    "desc": "compact-guide-to-large-language-models - Free download as PDF File (.pdf), Text File (.txt) or view presentation slides online. The document provides an introduction to large language models (LLMs), including a definition and brief history of their development. It discusses how LLMs work by analyzing vast amounts of natural language data to model human language. Recent developments, like increased computational power and improved training techniques, have led to significant performance gains and",
    "title": "Compact Guide To Large Language Models | PDF | Artificial Intelligence | Intelligence (AI) &amp; Semantics",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://scribe.rip/@msayef/llm-terminologies-decoded-a-beginners-quick-reference-guide-925f35794198",
    "desc": "LLM Terminologies Decoded: A Beginner&rsquo;s Quick Reference Guide",
    "title": "LLM Terminologies Decoded: A Beginner&rsquo;s Quick Reference Guide",
    "auth": "Sayef",
    "date": null
  },
  {
    "url": "https://scribe.rip/tales-of-tomorrow/a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0",
    "desc": "A Quiet Revolution? Mistral AI Releases Sensational New AI Model",
    "title": "A Quiet Revolution? Mistral AI Releases Sensational New AI Model",
    "auth": "Tristan Wolff",
    "date": 986079600
  },
  {
    "url": "https://inwaves.io/assets/gpt3rnn.pdf",
    "desc": "Cannot extract meta-data from PDF at present",
    "title": "gpt3rnn.pdf",
    "auth": "Unknown",
    "date": 0
  },
  {
    "url": "https://ai.plainenglish.io/fine-tuning-large-language-models-for-decision-support-a-comprehensive-guide-c1ef4c06abc6",
    "desc": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "title": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "auth": "unknown",
    "date": 0
  }
]