[
  {
    "url": "https://arxiv.org/abs/2005.11401",
    "desc": "Abstract page for arXiv paper 2005.11401: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "title": "[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://research.ibm.com/blog/retrieval-augmented-generation-RAG",
    "desc": "RAG is an AI framework for retrieving facts to ground LLMs on the most accurate information and to give users insight into AI&rsquo;s decision making process.",
    "title": "What is retrieval-augmented generation (RAG)? - IBM Research",
    "auth": "@IBMResearch",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2212.05773",
    "desc": "Abstract page for arXiv paper 2212.05773: A Survey on Natural Language Processing for Programming",
    "title": "[2212.05773] A Survey on Natural Language Processing for Programming",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2307.02503",
    "desc": "Abstract page for arXiv paper 2307.02503: Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "title": "[2307.02503] Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Dive into our blog for advanced strategies like ThoT, CoN, and CoVe to minimize hallucinations in RAG applications. Explore emotional prompts and ExpertPrompting to enhance LLM performance. Stay ahead in the dynamic RAG landscape with reliable insights for precise language models. Read now for a deep dive into refining LLMs.",
    "title": "Mastering RAG: LLM Prompting Techniques For Reducing Hallucinations",
    "auth": "Pratik Bhavsar",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2311.09210",
    "desc": "Abstract page for arXiv paper 2311.09210: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "title": "[2311.09210] Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://en.wikipedia.org/wiki/Backpropagation",
    "desc": "Backpropagation - Wikipedia",
    "title": "Backpropagation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/1706.03762",
    "desc": "Abstract page for arXiv paper 1706.03762: Attention Is All You Need",
    "title": "[1706.03762] Attention Is All You Need",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://scribe.rip/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6",
    "desc": "Using LLM&rsquo;s for Retrieval and Reranking",
    "title": "Using LLM&rsquo;s for Retrieval and Reranking",
    "auth": "Jerry Liu",
    "date": null
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models",
    "desc": "machine learning - What is the temperature\" in the GPT models? - Artificial Intelligence Stack Exchange",
    "title": "machine learning - What is the temperature\" in the GPT models? - Artificial Intelligence Stack Exchange",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275",
    "desc": "Understanding Dropout with the Simplified Math behind it",
    "title": "Understanding Dropout with the Simplified Math behind it",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://ar5iv.labs.arxiv.org/html/2310.04415",
    "desc": "[2310.04415] Why Do We Need Weight Decay in Modern Deep Learning?",
    "title": "[2310.04415] Why Do We Need Weight Decay in Modern Deep Learning?",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://symbl.ai/developers/blog/a-guide-to-llm-hyperparameters/",
    "desc": "A Guide to LLM Hyperparameters | Symbl.ai",
    "title": "A Guide to LLM Hyperparameters | Symbl.ai",
    "auth": "Kartik Talamadupula",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@mccartni/implications-of-batch-size-on-llm-training-and-inference-3320cb48d610",
    "desc": "Implications of Batch Size on LLM training and inference",
    "title": "Implications of Batch Size on LLM training and inference",
    "auth": "Nick McCarthy",
    "date": null
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://www.rungalileo.io/blog/mastering-rag-llm-prompting-techniques-for-reducing-hallucinations",
    "desc": "Dive into our blog for advanced strategies like ThoT, CoN, and CoVe to minimize hallucinations in RAG applications. Explore emotional prompts and ExpertPrompting to enhance LLM performance. Stay ahead in the dynamic RAG landscape with reliable insights for precise language models. Read now for a deep dive into refining LLMs.",
    "title": "Mastering RAG: LLM Prompting Techniques For Reducing Hallucinations",
    "auth": "Pratik Bhavsar",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/topics/ai-model",
    "desc": "An AI model is a program that applies one or more algorithms to data to recognize patterns, make predictions or make decisions without human intervention.",
    "title": "What Is an AI Model? | IBM",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gradient",
    "desc": "Gradient - Wikipedia",
    "title": "Gradient - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "desc": "Machine learning - Wikipedia",
    "title": "Machine learning - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Logistic_regression",
    "desc": "Logistic regression - Wikipedia",
    "title": "Logistic regression - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier",
    "desc": "Naive Bayes classifier - Wikipedia",
    "title": "Naive Bayes classifier - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Neural_network_%28machine_learning%29",
    "desc": "Neural network (machine learning) - Wikipedia",
    "title": "Neural network (machine learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Neural_scaling_law",
    "desc": "Neural scaling law - Wikipedia",
    "title": "Neural scaling law - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Byte_pair_encoding",
    "desc": "Byte pair encoding - Wikipedia",
    "title": "Byte pair encoding - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2302.01560v2",
    "desc": "HTTP_ERROR, Error: Timeout was reached",
    "title": "HTTP_ERROR, Error: Timeout was reached",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://github.com/AaronWard/generative-ai-workbook/discussions/19",
    "desc": "Reflexion: Language Agents with Verbal Reinforcement Learning",
    "title": "Reflexion: Language Agents with Verbal Reinforcement Learning &middot; AaronWard/generative-ai-workbook &middot; Discussion #19 &middot; GitHub",
    "auth": "AaronWard",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Rectified_flow",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Score_matching",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Monte_Carlo_tree_search",
    "desc": "Monte Carlo tree search - Wikipedia",
    "title": "Monte Carlo tree search - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Thermodynamic_beta",
    "desc": "Thermodynamic beta - Wikipedia",
    "title": "Thermodynamic beta - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Convolution",
    "desc": "Convolution - Wikipedia",
    "title": "Convolution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Stochastic",
    "desc": "Stochastic - Wikipedia",
    "title": "Stochastic - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mathematical_induction",
    "desc": "Mathematical induction - Wikipedia",
    "title": "Mathematical induction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Deterministic_algorithm",
    "desc": "Deterministic algorithm - Wikipedia",
    "title": "Deterministic algorithm - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",
    "desc": "Kullback&ndash;Leibler divergence - Wikipedia",
    "title": "Kullback&ndash;Leibler divergence - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Normal_distribution",
    "desc": "Normal distribution - Wikipedia",
    "title": "Normal distribution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Cosine_similarity",
    "desc": "Cosine similarity - Wikipedia",
    "title": "Cosine similarity - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/building-the-open-data-stack/a-vector-primer-understanding-the-basics-of-generative-ai-28ff4c1934a7",
    "desc": "A Vector Primer: Understanding the Basics of Generative AI",
    "title": "A Vector Primer: Understanding the Basics of Generative AI",
    "auth": "DataStax",
    "date": null
  },
  {
    "url": "https://en.wikipedia.org/wiki/Vector_notation",
    "desc": "Vector notation - Wikipedia",
    "title": "Vector notation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gaussian_splatting",
    "desc": "Gaussian splatting - Wikipedia",
    "title": "Gaussian splatting - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inverted_index",
    "desc": "Inverted index - Wikipedia",
    "title": "Inverted index - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/similarity-search-with-ivfpq-9c6348fd4db3",
    "desc": "Similarity Search with IVFPQ",
    "title": "Similarity Search with IVFPQ",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Cell-probe_model#Approximate_Nearest_Neighbor_Searching",
    "desc": "Cell-probe model - Wikipedia",
    "title": "Cell-probe model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/html/2404.19284v2",
    "desc": "Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation",
    "title": "Approximate Nearest Neighbour Search on Dynamic Datasets: An Investigation",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.elastic.co/blog/understanding-ann",
    "desc": "Discover the power of approximate nearest neighbor (ANN) algorithms. ANN search is the key to lightning-fast searches and valuable insights in vast data landscapes....",
    "title": "Understanding the approximate nearest neighbor (ANN) algorithm | Elastic Blog",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://ieeexplore.ieee.org/document/9942356/metrics#metrics",
    "desc": "Deep Learning for Approximate Nearest Neighbour Search: A Survey and Future Directions | IEEE Journals &amp; Magazine | IEEE Xplore",
    "title": "Deep Learning for Approximate Nearest Neighbour Search: A Survey and Future Directions | IEEE Journals &amp; Magazine | IEEE Xplore",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world",
    "desc": "Hierarchical navigable small world - Wikipedia",
    "title": "Hierarchical navigable small world - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://sds-aau.github.io/M3Port19/portfolio/ann/",
    "desc": "Approximate Nearest Neighbors Oh Yeah (ANNOY) - AAU Social Data Science Deep Learning - 2019 Portfolio",
    "title": "Approximate Nearest Neighbors Oh Yeah (ANNOY) - AAU Social Data Science Deep Learning - 2019 Portfolio",
    "auth": "M3Port19",
    "date": 0
  },
  {
    "url": "https://medium.com/@abdullahsamilguser/approximate-nearest-neighbor-methods-713dcfa8518f",
    "desc": "Approximate Nearest Neighbor Methods",
    "title": "Approximate Nearest Neighbor Methods",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://www.algolia.com/blog/ai/what-is-vector-search/",
    "desc": "What is vector search? - Algolia Blog | Algolia",
    "title": "What is vector search? - Algolia Blog | Algolia",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/gsi-technology/ann-benchmarks-a-data-scientists-journey-to-billion-scale-performance-db191f043a27",
    "desc": "ANN Benchmarks: A Data Scientist&rsquo;s Journey to Billion Scale Performance",
    "title": "ANN Benchmarks: A Data Scientist&rsquo;s Journey to Billion Scale Performance",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://research.google/blog/announcing-scann-efficient-vector-similarity-search/",
    "desc": "Posted by Philip Sun, Software Engineer, Google Research Suppose one wants to search through a large dataset of literary works using queries that r...",
    "title": "Announcing ScaNN: Efficient Vector Similarity Search",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://research.google/blog/soar-new-algorithms-for-even-faster-vector-search-with-scann/",
    "desc": "SOAR: New algorithms for even faster vector search with ScaNN",
    "title": "SOAR: New algorithms for even faster vector search with ScaNN",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://cloud.google.com/blog/products/databases/understanding-the-scann-index-in-alloydb",
    "desc": "Based on established Google technology, the new ScaNN index in AlloyDB delivers higher performance than the HNSW index in standard PostgreSQL.",
    "title": "Understanding the SCaNN index in AlloyDB | Google Cloud Blog",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@DataPlayer/scalable-approximate-nearest-neighbour-search-using-googles-scann-and-facebook-s-faiss-3e84df25ba",
    "desc": "State-of-the-Art Approximate Nearest Neighbor Search with Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS",
    "title": "State-of-the-Art Approximate Nearest Neighbor Search with Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2401.08281",
    "desc": "2401.08281",
    "title": "2401.08281",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/understanding-faiss-619bb6db2d1a",
    "desc": "Understanding FAISS",
    "title": "Understanding FAISS",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@DataPlayer/scalable-approximate-nearest-neighbour-search-using-googles-scann-and-facebook-s-faiss-3e84df25ba",
    "desc": "State-of-the-Art Approximate Nearest Neighbor Search with Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS",
    "title": "State-of-the-Art Approximate Nearest Neighbor Search with Google&rsquo;s ScaNN and Facebook&rsquo;s FAISS",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://github.com/yahoojapan/NGT/wiki/Command-Quick-Referenc",
    "desc": "Nearest Neighbor Search with Neighborhood Graph and Tree for High-dimensional Data - Home &middot; yahoojapan/NGT Wiki",
    "title": "Home &middot; yahoojapan/NGT Wiki &middot; GitHub",
    "auth": "yahoojapan",
    "date": 0
  },
  {
    "url": "https://opensource.com/article/19/10/ngt-open-source-library",
    "desc": "Approximate nearest neighbor (ANN) search is used in deep l",
    "title": "NGT: A library for high-speed approximate nearest neighbor search | Opensource.com",
    "auth": "Masajiro Iwasaki",
    "date": 0
  },
  {
    "url": "https://dl.acm.org/doi/abs/10.5555/3504035.3505114",
    "desc": "Fast approximate nearest neighbor search via k-diverse nearest neighbor graph | Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence",
    "title": "Fast approximate nearest neighbor search via k-diverse nearest neighbor graph | Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://spcl.inf.ethz.ch/Publications/.pdf/wenqi_fanns_slides.pdf",
    "desc": "HTTP_ERROR, Error: Timeout was reached",
    "title": "HTTP_ERROR, Error: Timeout was reached",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://dl.acm.org/doi/10.1145/3581784.3607045",
    "desc": "Co-design Hardware and Algorithm for Vector Search | Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
    "title": "Co-design Hardware and Algorithm for Vector Search | Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Hamming_distance",
    "desc": "Hamming distance - Wikipedia",
    "title": "Hamming distance - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Frobenius_inner_product",
    "desc": "Frobenius inner product - Wikipedia",
    "title": "Frobenius inner product - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Dot_product",
    "desc": "Dot product - Wikipedia",
    "title": "Dot product - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inner_product_space",
    "desc": "Inner product space - Wikipedia",
    "title": "Inner product space - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mahalanobis_distance",
    "desc": "Mahalanobis distance - Wikipedia",
    "title": "Mahalanobis distance - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://lantern.dev/blog/pq",
    "desc": "We implemented product quantization in Lantern and benchmarked it using the LAION 100M 768-dimensional vector dataset.",
    "title": "Product Quantization in Postgres | Lantern Blog",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.pinecone.io/learn/series/faiss/product-quantization/",
    "desc": "Product Quantization: Compressing high-dimensional vectors by 97% | Pinecone",
    "title": "Product Quantization: Compressing high-dimensional vectors by 97% | Pinecone",
    "auth": "@pinecone",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Locality-sensitive_hashing",
    "desc": "Locality-sensitive hashing - Wikipedia",
    "title": "Locality-sensitive hashing - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/K-d_tree",
    "desc": "k-d tree - Wikipedia",
    "title": "k-d tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://ieeexplore.ieee.org/document/8807277",
    "desc": "Fast and Scalable Approaches to Accelerate the Fuzzy k-Nearest Neighbors Classifier for Big Data | IEEE Journals &amp; Magazine | IEEE Xplore",
    "title": "Fast and Scalable Approaches to Accelerate the Fuzzy k-Nearest Neighbors Classifier for Big Data | IEEE Journals &amp; Magazine | IEEE Xplore",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Ball_tree",
    "desc": "Ball tree - Wikipedia",
    "title": "Ball tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://medium.com/@ravuri.saitarun/kd-tree-ball-tree-algorithms-83271c4d1cd0",
    "desc": "KD-Tree &amp; Ball Tree Algorithms",
    "title": "KD-Tree &amp; Ball Tree Algorithms",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@polkamnithyasri/kd-tree-and-ball-tree-algorithms-aa99dfad4ceb",
    "desc": "Kd -Tree and Ball Tree Algorithms",
    "title": "Kd -Tree and Ball Tree Algorithms",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://proceedings.neurips.cc/paper_files/paper/2004/file/1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "desc": "1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "title": "1102a326d5f7c9e04fc3c89d0ede88c9-Paper.pdf",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://project.inria.fr/gudhi/files/2017/04/clement-jamin.pdf",
    "desc": "clement-jamin.pdf",
    "title": "clement-jamin.pdf",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://patents.google.com/patent/US7539657",
    "desc": "One embodiment of the present invention provides a method and a system for building a parallel hybrid spill tree to facilitate parallel nearest-neighbor matching operations. During operation, the system receives a set of objects to be stored in the parallel hybrid spill tree. The system selects a subset of objects from the set of objects, and then uses this subset to create a &ldquo;top tree.&rdquo; Each node in the top tree defines an associated partition for the parallel hybrid, spill tree. The system use",
    "title": "US7539657B1 - Building parallel hybrid spill trees to facilitate parallel nearest-neighbor matching operations \n        - Google Patents",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/B%2B_tree",
    "desc": "B+ tree - Wikipedia",
    "title": "B+ tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/B-tree",
    "desc": "B-tree - Wikipedia",
    "title": "B-tree - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Robust_optimization",
    "desc": "Robust optimization - Wikipedia",
    "title": "Robust optimization - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2410.00690",
    "desc": "Abstract page for arXiv paper 2410.00690: Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "title": "[2410.00690] Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2406.16571",
    "desc": "Abstract page for arXiv paper 2406.16571: Differentiable Distributionally Robust Optimization Layers",
    "title": "[2406.16571] Differentiable Distributionally Robust Optimization Layers",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2401.14655",
    "desc": "Abstract page for arXiv paper 2401.14655: Distributionally Robust Optimization and Robust Statistics",
    "title": "[2401.14655] Distributionally Robust Optimization and Robust Statistics",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Continuous_function",
    "desc": "Continuous function - Wikipedia",
    "title": "Continuous function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time",
    "desc": "Discrete time and continuous time - Wikipedia",
    "title": "Discrete time and continuous time - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Markov_chain",
    "desc": "Markov chain - Wikipedia",
    "title": "Markov chain - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wasserstein_metric",
    "desc": "Wasserstein metric - Wikipedia",
    "title": "Wasserstein metric - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Uniform_convergence_in_probability",
    "desc": "Uniform convergence in probability - Wikipedia",
    "title": "Uniform convergence in probability - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Sigmoid_function",
    "desc": "Sigmoid function - Wikipedia",
    "title": "Sigmoid function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Logistic_function",
    "desc": "Logistic function - Wikipedia",
    "title": "Logistic function - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods",
    "desc": "Variational Bayesian methods - Wikipedia",
    "title": "Variational Bayesian methods - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Reparameterization_trick#Variational_inference",
    "desc": "Reparameterization trick - Wikipedia",
    "title": "Reparameterization trick - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://stackoverflow.com/questions/59868132/what-does-backbone-mean-in-a-neural-network#",
    "desc": "deep learning - What does backbone mean in a neural network? - Stack Overflow",
    "title": "deep learning - What does backbone mean in a neural network? - Stack Overflow",
    "auth": "No author for Q&amp;A sites",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Forward_diffusion_process",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Diffusion_model#Backward_diffusion_process_2",
    "desc": "Diffusion model - Wikipedia",
    "title": "Diffusion model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation",
    "desc": "Fokker&ndash;Planck equation - Wikipedia",
    "title": "Fokker&ndash;Planck equation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wiener_process",
    "desc": "Wiener process - Wikipedia",
    "title": "Wiener process - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Brownian_motion",
    "desc": "Brownian motion - Wikipedia",
    "title": "Brownian motion - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Langevin_equation",
    "desc": "Langevin equation - Wikipedia",
    "title": "Langevin equation - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Laplace_operator",
    "desc": "Laplace operator - Wikipedia",
    "title": "Laplace operator - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Discrete_mathematics",
    "desc": "Discrete mathematics - Wikipedia",
    "title": "Discrete mathematics - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Linearity",
    "desc": "Linearity - Wikipedia",
    "title": "Linearity - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.blopig.com/blog/2023/10/understanding-positional-encoding-in-transformers/",
    "desc": "Understanding positional encoding in Transformers | Oxford Protein Informatics Group",
    "title": "Understanding positional encoding in Transformers | Oxford Protein Informatics Group",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://ahmad-mustapha.scribe.rip/transformers-well-explained-positional-encoding-c8350904444f",
    "desc": "Transformers Well Explained: Positional Encoding",
    "title": "Transformers Well Explained: Positional Encoding",
    "auth": "Ahmad Mustapha",
    "date": 946684800
  },
  {
    "url": "https://scribe.rip/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b",
    "desc": "Positional Encoding Explained: A Deep Dive into Transformer PE",
    "title": "Positional Encoding Explained: A Deep Dive into Transformer PE",
    "auth": "Nikhil Chowdary Paleti",
    "date": 983404800
  },
  {
    "url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit",
    "desc": "Gated recurrent unit - Wikipedia",
    "title": "Gated recurrent unit - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Maurice_Tweedie",
    "desc": "Maurice Tweedie - Wikipedia",
    "title": "Maurice Tweedie - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution",
    "desc": "Inverse Gaussian distribution - Wikipedia",
    "title": "Inverse Gaussian distribution - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_statistics",
    "desc": "Maxwell&ndash;Boltzmann statistics - Wikipedia",
    "title": "Maxwell&ndash;Boltzmann statistics - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Brownian_motion",
    "desc": "Brownian motion - Wikipedia",
    "title": "Brownian motion - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Variational_Bayesian_methods",
    "desc": "Variational Bayesian methods - Wikipedia",
    "title": "Variational Bayesian methods - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/2401.12418",
    "desc": "Abstract page for arXiv paper 2401.12418: Towards Improved Variational Inference for Deep Bayesian Models",
    "title": "[2401.12418] Towards Improved Variational Inference for Deep Bayesian Models",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Scoring_rule#Hyv%C3%A4rinen_scoring_rule",
    "desc": "Scoring rule - Wikipedia",
    "title": "Scoring rule - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Simulated_annealing",
    "desc": "Simulated annealing - Wikipedia",
    "title": "Simulated annealing - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Annealing_%28materials_science%29",
    "desc": "Annealing (materials science) - Wikipedia",
    "title": "Annealing (materials science) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method",
    "desc": "Euler&ndash;Maruyama method - Wikipedia",
    "title": "Euler&ndash;Maruyama method - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Noisy_channel_model",
    "desc": "Noisy channel model - Wikipedia",
    "title": "Noisy channel model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@cloudswarup/the-building-blocks-of-llms-vectors-tokens-and-embeddings-1cd61cd20e35",
    "desc": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "title": "The Building Blocks of LLMs: Vectors, Tokens and Embeddings",
    "auth": "Swarup Das",
    "date": null
  },
  {
    "url": "https://cohere.com/blog/llm-parameters-best-outputs-language-ai",
    "desc": "When using Language AI to generate content, there are many options to control the outputs. Lets take a look at them in this post.",
    "title": "LLM Parameters Demystified: Getting The Best Outputs from Language AI",
    "auth": "@cohere",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Transformer_%28deep_learning_architecture%29",
    "desc": "Transformer (deep learning architecture) - Wikipedia",
    "title": "Transformer (deep learning architecture) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://byronsalty.scribe.rip/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "My (Growing) List of AI and LLM terminology",
    "title": "My (Growing) List of AI and LLM terminology",
    "auth": "Byron Salty",
    "date": 993942000
  },
  {
    "url": "https://en.wikipedia.org/wiki/Parametric_model",
    "desc": "Parametric model - Wikipedia",
    "title": "Parametric model - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.freecodecamp.org/news/retrieval-augmented-generation-rag-handbook/#2-2-parametric-vs-non-parametric-memory",
    "desc": "Retrieval Augmented Generation (RAG) signifies a transformative advancement in large language models (LLMs). It combines the generative prowess of transformer architectures with dynamic information retrieval.  This integration allows LLMs to access a...",
    "title": "Next-Gen Large Language Models: The Retrieval-Augmented Generation (RAG) Handbook",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://lagstill.scribe.rip/rag-analytics-explored-8d389978880f",
    "desc": "RAG Analytics Explored",
    "title": "RAG Analytics Explored",
    "auth": "Alagu Prakalya P",
    "date": 978307200
  },
  {
    "url": "https://scribe.rip/@crskilpatrick807/context-windows-the-short-term-memory-of-large-language-models-ab878fc6f9b5",
    "desc": "Context Windows: The Short-term Memory of Large Language Models",
    "title": "Context Windows: The Short-term Memory of Large Language Models",
    "auth": "Chandler K",
    "date": 991350000
  },
  {
    "url": "https://swimm.io/learn/large-language-models/llm-context-windows-basics-examples-and-prompting-best-practices",
    "desc": "A context window refers to the amount of text data a language model can consider at one time when generating responses.",
    "title": "LLM Context Windows: Basics, Examples &amp; Prompting Best Practices",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://towardsai.net/p/data-science/pause-for-performance-the-guide-to-using-early-stopping-in-ml-and-dl-model-training",
    "desc": "Author(s): Shivamshinde Originally published on Towards AI. Photo by Aleksandr Kadykov on UnsplashTable of ContentIntroduction- What are Bias and Variance?- ...",
    "title": "Pause for Performance: The Guide to Using Early Stopping in ML and DL Model Training | Towards AI",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://news.ycombinator.com/item?id=35666201",
    "desc": "Finetuning Large Language Models | Hacker News",
    "title": "Finetuning Large Language Models | Hacker News",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.whytryai.com/p/zero-shot-one-shot-few-shot-prompting",
    "desc": "Zero-Shot, One-Shot, Few-Shot, More?",
    "title": "Zero-Shot, One-Shot, Few-Shot, More?",
    "auth": "Daniel Nest",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Confusion_matrix",
    "desc": "Confusion matrix - Wikipedia",
    "title": "Confusion matrix - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://davpinto.github.io/fastknn/articles/knn-extraction.html",
    "desc": "Feature Extraction with KNN &bull; fastknn",
    "title": "Feature Extraction with KNN &bull; fastknn",
    "auth": "fastknn",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
    "desc": "Dimensionality reduction - Wikipedia",
    "title": "Dimensionality reduction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Data_reduction",
    "desc": "Data reduction - Wikipedia",
    "title": "Data reduction - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Decision_boundary",
    "desc": "Decision boundary - Wikipedia",
    "title": "Decision boundary - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Likelihood-ratio_test",
    "desc": "Likelihood-ratio test - Wikipedia",
    "title": "Likelihood-ratio test - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://datascience.fm/understanding-byte-pair-encoding-bpe-in-large-language-models-llms/",
    "desc": "This fixed vocabulary constraint becomes particularly problematic for languages with complex word formation processes like agglutination and compounding, which can create a vast number of word variations that a fixed-size vocabulary cant cover​​.",
    "title": "Understanding Byte Pair Encoding (BPE) in Large Language Models (LLMs)",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2306.08543v1",
    "desc": "2306.08543v1",
    "title": "2306.08543v1",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://betterprogramming.pub/large-language-model-knowledge-graph-store-yes-by-fine-tuning-llm-with-kg-f88b556959e6",
    "desc": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "title": "Large Language Model = Knowledge Graph Store? Yes, by Fine-Tuning LLM With KG",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://nlp.stanford.edu/projects/glove/",
    "desc": "GloVe: Global Vectors for Word Representation",
    "title": "GloVe: Global Vectors for Word Representation",
    "auth": "Jeffrey Pennington",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
    "desc": "Recurrent neural network - Wikipedia",
    "title": "Recurrent neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
    "desc": "Convolutional neural network - Wikipedia",
    "title": "Convolutional neural network - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Long_short-term_memory",
    "desc": "Long short-term memory - Wikipedia",
    "title": "Long short-term memory - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://www.ibm.com/think/topics/parameter-efficient-fine-tuning",
    "desc": "Parameter-efficient fine-tuning (PEFT) is a method of improving the performance of pretrained large language models (LLMs) and neural networks for specific tasks or data sets.",
    "title": "What is parameter-efficient fine-tuning (PEFT)?",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://wiki.civitai.com/wiki/Low-Rank_Adaptation",
    "desc": "Low-Rank Adaptation - Civitai Wiki",
    "title": "Low-Rank Adaptation - Civitai Wiki",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Fine-tuning_%28deep_learning%29#Low-rank_adaptation",
    "desc": "Fine-tuning (deep learning) - Wikipedia",
    "title": "Fine-tuning (deep learning) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/BERT_%28language_model%29",
    "desc": "BERT (language model) - Wikipedia",
    "title": "BERT (language model) - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
    "desc": "Mixture of experts - Wikipedia",
    "title": "Mixture of experts - Wikipedia",
    "auth": "Wikipedia contributors",
    "date": 0
  },
  {
    "url": "https://blog.svenson.ai/step-back-prompting-a-new-technique-for-abstraction-and-reasoning-in-large-language-models",
    "desc": "Step-Back Prompting is an innovative technique that enables large language models to perform complex reasoning tasks",
    "title": "Step-Back Prompting: A New Technique for Abstraction and Reasoning in Large Language Models",
    "auth": "Svenson.ai",
    "date": 0
  },
  {
    "url": "https://scribe.rip/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83",
    "desc": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "title": "Rotary Positional Embeddings: A Detailed Look and Comprehensive Understanding",
    "auth": "azhar",
    "date": null
  },
  {
    "url": "https://paperswithcode.com/method/ulmfit",
    "desc": "Universal Language Model Fine-tuning, or ULMFiT, is an architecture and transfer learning method that can be applied to NLP tasks. It involves a 3-layer AWD-LSTM architecture for its representations. The training consists of three steps: 1) general language model pre-training on a Wikipedia-based text, 2) fine-tuning the language model on a target task, and 3) fine-tuning the classifier on the target task.\n\nAs different layers capture different types of information, they are fine-tuned to differ",
    "title": "ULMFiT Explained | Papers With Code",
    "auth": "@paperswithcode",
    "date": 0
  },
  {
    "url": "https://paperswithcode.com/method/xlnet",
    "desc": "XLNet is an autoregressive Transformer that leverages the best of both autoregressive language modeling and autoencoding while attempting to avoid their limitations. Instead of using a fixed forward or backward factorization order as in conventional autoregressive models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and ",
    "title": "XLNet Explained | Papers With Code",
    "auth": "@paperswithcode",
    "date": 0
  },
  {
    "url": "https://epochai.org/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data",
    "desc": "We estimate the stock of human-generated public text at around 300 trillion tokens. If trends continue, language models will fully utilize this stock between 2026 and 2032, or even earlier if intensely overtrained.",
    "title": "Will We Run Out of Data to Train Large Language Models? | Epoch AI",
    "auth": "Pablo Villalobos",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2211.04325",
    "desc": "2211.04325",
    "title": "2211.04325",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://www.fast.ai/posts/2023-09-04-learning-jumps/",
    "desc": "We&rsquo;ve noticed an unusual training pattern in fine-tuning LLMs. At first we thought it&rsquo;s a bug, but now we think it shows LLMs can learn effectively from a single example.",
    "title": "Can LLMs learn from a single example? &ndash; fast.ai",
    "auth": "Jeremy Howard and Jonathan Whitaker",
    "date": 0
  },
  {
    "url": "https://www.coursera.org/collections/llms-terms",
    "desc": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "title": "Large Language Models (LLMs) Definitions : A to Z Glossary Terms",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://byronsalty.scribe.rip/my-growing-list-of-ai-and-llm-terminology-26d8b109a14f",
    "desc": "My (Growing) List of AI and LLM terminology",
    "title": "My (Growing) List of AI and LLM terminology",
    "auth": "Byron Salty",
    "date": 993942000
  },
  {
    "url": "https://toloka.ai/blog/history-of-llms/",
    "desc": "The impressive speed at which AI has evolved has never been more apparent than it is now, with ChatGPT making headlines and the dramatic evolution of Large Language Models (LLMs) ever present in the media cycle. Millions of people worldwide have wasted no time adopting conversational AI tools in their day-to-day existence. These tools have not only enamored but also terrified audiences with their striking capabilities and efficiency and their potentially dangerous implications if not regulated w",
    "title": "The history, timeline, and future of LLMs",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.meetgrit.com/s/articles/a06Qp00000C09dtIAB/a-compact-guide-to-large-language-models-defining-llms-and-their-role-in-ai",
    "desc": "Grit Holdings",
    "title": "Grit Holdings",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://scribe.rip/@rtales/tuning-parameters-to-train-llms-large-language-models-8861bbc11971",
    "desc": "Tuning parameters to train LLMs (Large Language Models)",
    "title": "Tuning parameters to train LLMs (Large Language Models)",
    "auth": "Tales Matos",
    "date": 946684800
  },
  {
    "url": "https://scribe.rip/@msayef/llm-terminologies-decoded-a-beginners-quick-reference-guide-925f35794198",
    "desc": "LLM Terminologies Decoded: A Beginner&rsquo;s Quick Reference Guide",
    "title": "LLM Terminologies Decoded: A Beginner&rsquo;s Quick Reference Guide",
    "auth": "Sayef",
    "date": null
  },
  {
    "url": "https://scribe.rip/tales-of-tomorrow/a-quiet-revolution-mistral-ai-releases-sensational-new-ai-model-c17c663287f0",
    "desc": "A Quiet Revolution? Mistral AI Releases Sensational New AI Model",
    "title": "A Quiet Revolution? Mistral AI Releases Sensational New AI Model",
    "auth": "Tristan Wolff",
    "date": 986079600
  },
  {
    "url": "https://towardsdatascience.com/gpt-3-rnns-and-all-that-deep-dive-into-language-modelling-7f67658ba0d5",
    "desc": "GPT-3, RNNs and All That: A Deep Dive into Language Modelling",
    "title": "GPT-3, RNNs and All That: A Deep Dive into Language Modelling",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://ai.plainenglish.io/fine-tuning-large-language-models-for-decision-support-a-comprehensive-guide-c1ef4c06abc6",
    "desc": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "title": "Fine-Tuning Large Language Models for Decision Support: A Comprehensive Guide",
    "auth": "unknown",
    "date": 0
  }
]