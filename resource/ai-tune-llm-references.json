[
  {
    "url": "https://situational-awareness.ai/from-gpt-4-to-agi/",
    "desc": "AGI by 2027 is strikingly plausible. GPT-2 to GPT-4 took us from ~preschooler to ~smart high-schooler abilities in 4 years. Tracing trendlines in compute (~0.5 orders of magnitude or OOMs/year), algorithmic efficiencies (~0.5 OOMs/year), and &ldquo;unhobbling&rdquo; gains (from chatbot to agent), we should expect another preschooler-to-high-schooler-sized qualitative jump by 2027. Look. The models, they just",
    "title": "I. From GPT-4 to AGI: Counting the OOMs - SITUATIONAL AWARENESS",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://angelxuanchang.github.io/nlp-class/assets/lecture-slides/scaling.pdf",
    "desc": "scaling.pdf",
    "title": "scaling.pdf",
    "auth": "nlp-class",
    "date": 0
  },
  {
    "url": "https://www.fast.ai/posts/2023-09-04-learning-jumps/",
    "desc": "We&rsquo;ve noticed an unusual training pattern in fine-tuning LLMs. At first we thought it&rsquo;s a bug, but now we think it shows LLMs can learn effectively from a single example.",
    "title": "Can LLMs learn from a single example? &ndash; fast.ai",
    "auth": "Jeremy Howard and Jonathan Whitaker",
    "date": 0
  },
  {
    "url": "https://arxiv.org/abs/1712.09913",
    "desc": "Abstract page for arXiv paper 1712.09913: Visualizing the Loss Landscape of Neural Nets",
    "title": "[1712.09913] Visualizing the Loss Landscape of Neural Nets",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2402.06196",
    "desc": "2402.06196",
    "title": "2402.06196",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://scalexm.ai/storage/2023/10/White-Paper-LLM.pdf",
    "desc": "White-Paper-LLM.pdf",
    "title": "White-Paper-LLM.pdf",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://medium.com/version-1/running-your-own-dedicated-openai-instance-60a93555dbd0",
    "desc": "Running your own dedicated OpenAI Instance",
    "title": "Running your own dedicated OpenAI Instance",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://sebastianraschka.com/blog/2024/building-a-gpt-style-llm-classifier.html",
    "desc": "Im an LLM Research Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM research and the development of high-performance AI systems, with a deep focus on practical, code-driven implementations.",
    "title": "Building A GPT-Style LLM Classifier From Scratch",
    "auth": "Sebastian Raschka",
    "date": 0
  },
  {
    "url": "https://cohere.com/blog/llm-parameters-best-outputs-language-ai",
    "desc": "When using Language AI to generate content, there are many options to control the outputs. Lets take a look at them in this post.",
    "title": "LLM Parameters Demystified: Getting The Best Outputs from Language AI",
    "auth": "@cohere",
    "date": 0
  },
  {
    "url": "https://towardsdatascience.com/different-ways-of-training-llms-c57885f388ed",
    "desc": "Different ways of training LLMs | Towards Data Science",
    "title": "Different ways of training LLMs | Towards Data Science",
    "auth": "Dorian Drost",
    "date": 0
  },
  {
    "url": "https://www.semianalysis.com/p/google-we-have-no-moat-and-neither",
    "desc": "Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI The text below is a very recent leaked document, which was shared by an anonymous individual on a public Discord server who has granted permission for its republication. It originates from a researcher within Google. We have verified its authenticity. The only&hellip;",
    "title": "Google &ldquo;We Have No Moat, And Neither Does OpenAI&rdquo; &ndash; SemiAnalysis",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://artificialanalysis.ai/models/gpt-4o-2024-08-06",
    "desc": "Analysis of OpenAIs GPT-4o (Aug '24) and comparison to other AI models across key metrics including quality, price, performance (tokens per second &amp; time to first token), context window &amp; more.",
    "title": "GPT-4o (Aug 24) - Intelligence, Performance &amp; Price Analysis | Artificial Analysis",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://github.blog/ai-and-ml/llms/customizing-and-fine-tuning-llms-what-you-need-to-know/",
    "desc": "Learn how your organization can customize its LLM-based solution through retrieval augmented generation and fine-tuning.",
    "title": "Customizing and fine-tuning LLMs: What you need to know - The GitHub Blog",
    "auth": "Nicole Choi",
    "date": 0
  },
  {
    "url": "https://www.acorn.io/resources/learning-center/fine-tuning-llm",
    "desc": "Fine-tuning Large Language Models (LLMs) involves adjusting pre-trained models on specific datasets to enhance performance for particular tasks.",
    "title": "Fine-Tuning LLMs: Top 6 Methods, Challenges &amp; Best Practices",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://deepchecks.com/how-to-test-llm-applications-before-releasing-to-production/",
    "desc": "Validating a Large Language Model is a challenging task. Learn to test LLMs to form a production-ready application.",
    "title": "How to Test LLM Applications Before Releasing to Production",
    "auth": "Deepchecks Community Blog",
    "date": 0
  },
  {
    "url": "https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/",
    "desc": "Mixture of experts (MoE) large language model (LLM) architectures have recently emerged, both in proprietary LLMs such as GPT-4, as well as in community models&hellip;",
    "title": "Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://huggingface.co/docs/transformers/model_doc/mixtral",
    "desc": "We&rsquo;re on a journey to advance and democratize artificial intelligence through open source and open science.",
    "title": "Mixtral",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://medium.com/@malachy.moran/llm-reading-list-understanding-attention-is-all-you-need-part-i-4cba140bd541",
    "desc": "LLM Reading List: Understanding &ldquo;Attention Is All You Need&rdquo; Part I",
    "title": "LLM Reading List: Understanding &ldquo;Attention Is All You Need&rdquo; Part I",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://medium.com/@harikapanuganty/from-words-to-vectors-inside-the-llm-transformer-architecture-50275c354bc4",
    "desc": "From Words to Vectors: Inside the LLM Transformer Architecture",
    "title": "From Words to Vectors: Inside the LLM Transformer Architecture",
    "auth": "cant extract from medium",
    "date": 0
  },
  {
    "url": "https://huggingface.co/blog/mlabonne/merge-models",
    "desc": "A Blog post by Maxime Labonne on Hugging Face",
    "title": "Merge Large Language Models with mergekit",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://www.thelancet.com/journals/ebiom/article/PIIS2352-3964(23)00077-4/fulltext",
    "desc": "HTTP_ERROR, Received code 403 code.",
    "title": "",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://datasciencedojo.com/blog/embeddings-and-llm/",
    "desc": "Explore the role of embeddings in large language models (LLMs). Learn how they power understanding, context, and representation in AI advancements.",
    "title": "Embeddings 101: The Foundation of LLM Power and Innovation",
    "auth": "@datasciencedojo",
    "date": 0
  },
  {
    "url": "https://www.turing.com/resources/finetuning-large-language-models",
    "desc": "Fine-tuning is the process of adjusting the parameters of a pre-trained LLM to a specific task or domain. Learn about the methods and how to fine-tune LLMs",
    "title": "What is Fine-Tuning LLM? Methods &amp; Step-by-Step Guide in 2025",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://writer.com/blog/how-to-evaluate-generative-ai-llm-vendors/",
    "desc": "Discover the key considerations, challenges, and best practices when evaluating LLM and generative AI vendors for enterprise solutions.",
    "title": "How to evaluate LLM vendors for enterprise solutions - Writer",
    "auth": "Matt Sobel",
    "date": 0
  },
  {
    "url": "https://machinelearning.apple.com/research/openelm",
    "desc": "This paper has been accepted at the Efficient Systems for Foundation Models workshop at ICML 2024.\nThe reproducibility and transparency of&hellip;",
    "title": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework - Apple Machine Learning Research",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/html/2404.14619v2",
    "desc": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
    "title": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://venturebeat.com/ai/apple-shows-off-open-ai-prowess-new-models-outperform-mistral-and-hugging-face-offerings/",
    "desc": "New Apple model delivers nearly similar performance to leading open models, including Mistral-7B, Llama3 8B and Google&rsquo;s Gemma",
    "title": "Apple shows off open AI prowess: new models outperform Mistral and Hugging Face offerings | VentureBeat",
    "auth": "Shubham Sharma",
    "date": 0
  },
  {
    "url": "https://llm.datasette.io/en/stable/",
    "desc": "LLM: A CLI utility and Python library for interacting with Large Language Models",
    "title": "LLM: A CLI utility and Python library for interacting with Large Language Models",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/html/2408.13296v1",
    "desc": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)",
    "title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)",
    "auth": "unknown",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2406.03505",
    "desc": "2406.03505",
    "title": "2406.03505",
    "auth": "",
    "date": 0
  },
  {
    "url": "https://arxiv.org/pdf/2302.09210",
    "desc": "2302.09210",
    "title": "2302.09210",
    "auth": "unknown",
    "date": 0
  }
]